{
  "metadata": {
    "kernelspec": {
      "name": "SQLite",
      "display_name": "SQLite",
      "language": "sql"
    },
    "language_info": {
      "codemirror_mode": "sql",
      "file_extension": "",
      "mimetype": "",
      "name": "sql",
      "version": "3.32.3"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "%%timeit\n#finalCode \nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\n\n\n# Rastgele sayı üreteci ayarla\nSEED = 180401027\ntorch.manual_seed(SEED)\nfrom sklearn.preprocessing import MinMaxScaler,RobustScaler\nskaler = MinMaxScaler()\n# Verileri tensor'lara dönüştür\ntrain_X = torch.tensor(train_data.values[:, :-1], dtype=torch.float32)\ntrain_X = torch.tensor(skaler.fit_transform(train_X),dtype=torch.float32)\ntrain_y = torch.tensor(train_data.values[:, -1], dtype=torch.float32)\nval_X = torch.tensor(val_data.values[:, :-1], dtype=torch.float32)\nval_X = torch.tensor(skaler.fit_transform(val_X),dtype=torch.float32)\nval_y = torch.tensor(val_data.values[:, -1], dtype=torch.float32)\ntest_X = torch.tensor(test_data.values[:, :-1], dtype=torch.float32)\ntest_X = torch.tensor(skaler.fit_transform(test_X),dtype=torch.float32)\ntest_y = torch.tensor(test_data.values[:, -1], dtype=torch.float32)\ntrain_y = train_y.reshape(-1, 1)\ntest_y = test_y.reshape(-1, 1)\nval_y = val_y.reshape(-1, 1)",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "976801cf-192a-4c9b-96f4-0682ad552975"
    },
    {
      "cell_type": "code",
      "source": "%load_ext autotime\n# MLP modelini tanımla\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nclass MLP(nn.Module):\n    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n        super(MLP, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size1)\n        self.relu1 = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n        self.relu2 = nn.ReLU()\n        self.fc3 = nn.Linear(hidden_size2, output_size)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu1(out)\n        out = self.fc2(out)\n        out = self.relu2(out)\n        out = self.fc3(out)\n        out = self.sigmoid(out)\n        return out\n\n# Modeli eğit\ninput_size = train_X.shape[1]\nhidden_size1 = 100\nhidden_size2 = 50\noutput_size = 1\n\nmodel = MLP(input_size, hidden_size1, hidden_size2, output_size)\n\ncriterion = nn.BCELoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001)\nbatch_size = 16\nnum_epochs = 700\npatience = 10\n\n# Eğitim ve doğrulama seti için kayıp değerlerini tutacak listeleri oluştur\ntrain_loss_list = []\nval_loss_list = []\nbest_val_loss=None\npatience_counter=0\n# Eğitim döngüsü\nfor epoch in range(num_epochs):\n    # Batch'lerden eğitim kayıplarını tutacak bir liste oluştur\n    epoch_train_loss = []\n    for i in range(0, len(train_X), batch_size):\n        inputs = train_X[i:i+batch_size]\n        labels = train_y[i:i+batch_size].reshape(-1, 1)\n\n        optimizer.zero_grad()\n\n        outputs = model(inputs)\n\n        loss = criterion(outputs, labels)\n        loss.backward()\n\n        optimizer.step()\n\n        # Batch kaybını ekle\n        epoch_train_loss.append(loss.item())\n\n    # Eğitim kaybını epoch sonunda hesapla ve listeye ekle\n    train_loss = np.mean(epoch_train_loss)\n    train_loss_list.append(train_loss)\n\n # Doğrulama kaybını hesapla ve listeye ekle\n    val_loss = 0.0\n    \"\"\"val_correct = 0.0\n    val_total = 0.0\"\"\"\n    #validation_count = 0.0\n    with torch.no_grad():\n        model.eval()\n        for validation_X, validation_y in zip(val_X,val_y):\n            outputs = model(validation_X)\n            loss = criterion(outputs, validation_y)\n            #validation_count += 1.0\n            val_loss += loss.item()\n            \"\"\", predicted = torch.max(outputs, 1) # burada validation accuracy hesabı var.\n            val_total += labels.size(0)\n            val_correct += (predicted == labels).sum().item()\"\"\"\n\n    model.train()\n\n    # calculate metrics\n  \n    val_loss /= val_X.shape[0]\n\n    #val_acc = 100 * val_correct / val_total\n\n    print(\"Epoch\", epoch, \"Training loss\", train_loss,\"Validation Loss :\",val_loss)\n\n    \n    val_loss_list.append(val_loss)\n\n    val_score = val_loss\n    \n    if best_val_loss is None:\n        best_val_loss = val_score # hafızada patience boyu tutmaya başla\n        torch.save(model.state_dict(), \"checkpoint.pt\")\n    elif best_val_loss < val_score: # patience counter\n        patience_counter += 1\n        print(\"Earlystopping Patience Counter:\",patience_counter)\n        if patience_counter == patience:\n            break\n    else:\n        best_val_loss = val_score\n        torch.save(model.state_dict(), \"checkpoint.pt\") # to keep the best model\n        patience_counter = 0\nplt.plot(train_loss_list, label=\"Training loss\")\nplt.plot(val_loss_list, label=\"Validation loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()\n\n\nprint('Finished Training')\nmodel = MLP(input_size, hidden_size1, hidden_size2, output_size)\nmodel.load_state_dict(torch.load('checkpoint.pt'))\nmodel.eval()\npredicts =[]\nreal_labels = list()\nwith torch.no_grad():\n    for inputs,label in zip(test_X,test_y):\n        outputs = model(inputs)\n        predict = round(float(outputs.data))\n        predicts.append(predict)\n\n\n        real_labels.extend(label.tolist())\nfrom sklearn.metrics import f1_score,accuracy_score,classification_report\nprint(\"Accuracy score of this model: {}\".format(accuracy_score(real_labels,predicts)))\nprint(classification_report(real_labels,predicts))",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "2f8645df-225d-4b97-a0ba-f66f5889d75b"
    }
  ]
}